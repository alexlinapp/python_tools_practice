{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMolqU97hdvak5aPU3nN4Ba",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexlinapp/python_tools_practice/blob/main/cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJLo2dPcz1nx",
        "outputId": "4278e8aa-ec98-42c6-c697-4aa649a32b69"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3pCCg4c9MTw",
        "outputId": "21872b2b-c782-493a-d84a-bc7b3256f317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from fastai.vision.all import *\n",
        "from torchinfo import summary\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Download and load MNIST training data\n",
        "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Download and load MNIST test data\n",
        "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfty6PLw-Uje",
        "outputId": "4c346361-1447-4277-ee38-4bf8b7969985"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 17.8MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 518kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.66MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.76MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dls = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
        "xb, yb = next(iter(dls))\n",
        "xb = xb.view(64, -1, 28, 28)\n",
        "xb.shape, yb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76pcfq7Y-XEn",
        "outputId": "892ff98a-f0b4-4c5a-f799-5e96a788afbf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_edge = torch.tensor([[-1,-1,-1], [0,0,0], [1,1,1]]).float()\n",
        "bottom_edge = torch.tensor([[1,1,1], [0,0,0], [-1,-1,-1]]).float()\n",
        "edge_kernels = torch.stack([top_edge, bottom_edge]).unsqueeze(1)\n",
        "#show_image(xb[59].view(-1, 28))\n",
        "batch_features = F.conv2d(xb.view(-1, 1, 28, 28), edge_kernels, stride=1, padding=1)\n",
        "show_image(batch_features[59][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "3QV_LxcQ9RB3",
        "outputId": "38232013-7ef8-4802-c852-c77402ecb9e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 100x100 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "broken_cnn = sequential(\n",
        "    nn.Conv2d(1,30, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(30,1, kernel_size=3, padding=1)\n",
        ")\n",
        "broken_cnn(xb).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnSj9mGJldiS",
        "outputId": "78276df8-6b7a-45f3-f432-3f2ffa24aa90"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conv(ni, nf, ks=3, act=True):\n",
        "  res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n",
        "  if act: res = nn.Sequential(res, nn.ReLU())\n",
        "  return res\n",
        "def conv_with_batchnorm(ni, nf, ks=3, act=True):\n",
        "  layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)]\n",
        "  if act: layers.append(nn.ReLU())\n",
        "  layers.append(nn.BatchNorm2d(nf))\n",
        "  return nn.Sequential(*layers)\n",
        "\n",
        "# THIS IS STANDARD. Do not want to normalize after activations\n",
        "def conv_with_batchnorm_reversed(ni, nf, ks=3, act=True):\n",
        "  layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)]\n",
        "  layers.append(nn.BatchNorm2d(nf))\n",
        "  if act: layers.append(nn.ReLU())\n",
        "  return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "XdJZ82aMuxSJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_cnn = sequential(\n",
        "    conv(1 ,4),            #14x14\n",
        "    conv(4 ,8),            #7x7\n",
        "    conv(8 ,16),           #4x4\n",
        "    conv(16,32),           #2x2\n",
        "    conv(32,10, act=False), #1x1\n",
        "    Flatten(),\n",
        ")\n",
        "simple_cnn_v2 = sequential(\n",
        "    conv_with_batchnorm(1 ,8),            #14x14\n",
        "    conv_with_batchnorm(8 ,16),            #7x7\n",
        "    conv_with_batchnorm(16 ,32),           #4x4\n",
        "    conv_with_batchnorm(32,64),           #2x2\n",
        "    conv_with_batchnorm(64,10, act=False), #1x1\n",
        "    Flatten(),\n",
        ")\n",
        "simple_cnn_v3 = sequential(\n",
        "    conv_with_batchnorm_reversed(1 ,8),            #14x14\n",
        "    conv_with_batchnorm_reversed(8 ,16),            #7x7\n",
        "    conv_with_batchnorm_reversed(16 ,32),           #4x4\n",
        "    conv_with_batchnorm_reversed(32,64),           #2x2\n",
        "    conv_with_batchnorm_reversed(64,10, act=False), #1x1\n",
        "    Flatten(),\n",
        ")\n",
        "simple_cnn(xb).shape\n",
        "summary(simple_cnn, input_size=(64, 1, 28, 28))#, simple_cnn(xb).shape\n",
        "summary(simple_cnn_v2, input_size=(64, 1, 28, 28))#, simple_cnn(xb).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbF3AZDIyknX",
        "outputId": "d432e8d3-dea1-47df-81c0-9cde7880cd69"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Sequential                               [64, 10]                  --\n",
              "├─Sequential: 1-1                        [64, 8, 14, 14]           --\n",
              "│    └─Conv2d: 2-1                       [64, 8, 14, 14]           80\n",
              "│    └─ReLU: 2-2                         [64, 8, 14, 14]           --\n",
              "│    └─BatchNorm2d: 2-3                  [64, 8, 14, 14]           16\n",
              "├─Sequential: 1-2                        [64, 16, 7, 7]            --\n",
              "│    └─Conv2d: 2-4                       [64, 16, 7, 7]            1,168\n",
              "│    └─ReLU: 2-5                         [64, 16, 7, 7]            --\n",
              "│    └─BatchNorm2d: 2-6                  [64, 16, 7, 7]            32\n",
              "├─Sequential: 1-3                        [64, 32, 4, 4]            --\n",
              "│    └─Conv2d: 2-7                       [64, 32, 4, 4]            4,640\n",
              "│    └─ReLU: 2-8                         [64, 32, 4, 4]            --\n",
              "│    └─BatchNorm2d: 2-9                  [64, 32, 4, 4]            64\n",
              "├─Sequential: 1-4                        [64, 64, 2, 2]            --\n",
              "│    └─Conv2d: 2-10                      [64, 64, 2, 2]            18,496\n",
              "│    └─ReLU: 2-11                        [64, 64, 2, 2]            --\n",
              "│    └─BatchNorm2d: 2-12                 [64, 64, 2, 2]            128\n",
              "├─Sequential: 1-5                        [64, 10, 1, 1]            --\n",
              "│    └─Conv2d: 2-13                      [64, 10, 1, 1]            5,770\n",
              "│    └─BatchNorm2d: 2-14                 [64, 10, 1, 1]            20\n",
              "├─Flatten: 1-6                           [64, 10]                  --\n",
              "==========================================================================================\n",
              "Total params: 30,414\n",
              "Trainable params: 30,414\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 14.54\n",
              "==========================================================================================\n",
              "Input size (MB): 0.20\n",
              "Forward/backward pass size (MB): 3.21\n",
              "Params size (MB): 0.12\n",
              "Estimated Total Size (MB): 3.53\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, optimizer, epochs=4, device=\"cpu\"):\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    for xb, yb in DataLoader(mnist_train, batch_size=64, shuffle=True):\n",
        "      optimizer.zero_grad()\n",
        "      xb = xb.to(device)\n",
        "      yb = yb.to(device)\n",
        "      preds = model(xb)\n",
        "      loss = F.cross_entropy(preds, yb)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      correct += (preds.argmax(dim=1) == yb).sum().item()\n",
        "      train_loss += loss.item() * xb.size(0)\n",
        "    train_loss /= len(mnist_train)\n",
        "    train_acc = correct / len(mnist_train)\n",
        "    print(f\"Epoch {epoch+1}: Train loss {train_loss:.4f}, Train accuracy {train_acc:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "def validate_model(model, epochs = 1, device=\"cpu\"):\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  for epoch in range(epochs):\n",
        "    correct = 0\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "      for xb, yb in DataLoader(mnist_test, batch_size=64, shuffle=True):\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        preds = model(xb)\n",
        "        loss = F.cross_entropy(preds, yb)\n",
        "        val_loss += loss.item() * xb.size(0)\n",
        "        correct += (preds.argmax(dim=1) == yb).sum().item()\n",
        "    val_acc = correct / len(mnist_test)\n",
        "    val_loss /= len(mnist_test)\n",
        "    print(f\"Epoch {epoch+1}: Val loss {val_loss:.4f}, Val accuracy {val_acc:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "def train_and_validate(model, optimizer, epochs=4, device=\"cpu\"):\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    for xb, yb in DataLoader(mnist_train, batch_size=64, shuffle=True):\n",
        "      optimizer.zero_grad()\n",
        "      xb = xb.to(device)\n",
        "      yb = yb.to(device)\n",
        "      preds = model(xb)\n",
        "      loss = F.cross_entropy(preds, yb)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_correct += (preds.argmax(dim=1) == yb).sum().item()\n",
        "      train_loss += loss.item() * xb.size(0)\n",
        "    train_loss /= len(mnist_train)\n",
        "    train_acc = train_correct / len(mnist_train)\n",
        "    #print(f\"Epoch {epoch+1}: Train loss {train_loss:.4f}, Train accuracy {train_acc:.4f}\")\n",
        "    val_correct = 0\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "      for xb, yb in DataLoader(mnist_test, batch_size=64, shuffle=True):\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        preds = model(xb)\n",
        "        loss = F.cross_entropy(preds, yb)\n",
        "        val_loss += loss.item() * xb.size(0)\n",
        "        val_correct += (preds.argmax(dim=1) == yb).sum().item()\n",
        "    val_acc = val_correct / len(mnist_test)\n",
        "    val_loss /= len(mnist_test)\n",
        "    print(f\"Epoch {epoch+1}: Train loss {train_loss:.4f}, Train accuracy {train_acc:.4f} Val loss {val_loss:.4f}, Val accuracy {val_acc:.4f}\")\n",
        "\n",
        "def train_and_validate_adj_lr(model, optimizer, epochs=4, device=\"cpu\"):\n",
        "  model.to(device)\n",
        "  for epoch in range(epochs):\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    model.train()\n",
        "    for xb, yb in DataLoader(mnist_train, batch_size=64, shuffle=True):\n",
        "      optimizer.zero_grad()\n",
        "      xb = xb.to(device)\n",
        "      yb = yb.to(device)\n",
        "      preds = model(xb)\n",
        "      loss = F.cross_entropy(preds, yb)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_correct += (preds.argmax(dim=1) == yb).sum().item()\n",
        "      train_loss += loss.item() * xb.size(0)\n",
        "    train_loss /= len(mnist_train)\n",
        "    train_acc = train_correct / len(mnist_train)\n",
        "    #print(f\"Epoch {epoch+1}: Train loss {train_loss:.4f}, Train accuracy {train_acc:.4f}\")\n",
        "    val_correct = 0\n",
        "    val_loss = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for xb, yb in DataLoader(mnist_test, batch_size=64, shuffle=True):\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        preds = model(xb)\n",
        "        loss = F.cross_entropy(preds, yb)\n",
        "        val_loss += loss.item() * xb.size(0)\n",
        "        val_correct += (preds.argmax(dim=1) == yb).sum().item()\n",
        "    val_acc = val_correct / len(mnist_test)\n",
        "    val_loss /= len(mnist_test)\n",
        "    print(f\"Epoch {epoch+1}: Train loss {train_loss:.4f}, Train accuracy {train_acc:.4f} Val loss {val_loss:.4f}, Val accuracy {val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "5Gir9cpD0a1z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_model(simple_cnn, torch.optim.Adam(simple_cnn.parameters(), lr=0.01), epochs=5, device=\"cuda\")\n",
        "#validate_model(simple_cnn, epochs=5, device=\"cuda\")\n",
        "train_and_validate(simple_cnn, torch.optim.Adam(simple_cnn.parameters(), lr=0.01), epochs=5, device=\"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "I04Il2tU0e9m",
        "outputId": "3bbc889a-e68a-4572-9151-50435ffbe925"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss 0.2584, Train accuracy 0.9164 Val loss 0.1179, Val accuracy 0.9633\n",
            "Epoch 2: Train loss 0.1147, Train accuracy 0.9653 Val loss 0.1002, Val accuracy 0.9686\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-547258488.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#train_model(simple_cnn, torch.optim.Adam(simple_cnn.parameters(), lr=0.01), epochs=5, device=\"cuda\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#validate_model(simple_cnn, epochs=5, device=\"cuda\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_and_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-9-1969782447.py\u001b[0m in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, optimizer, epochs, device)\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m       \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_validate(simple_cnn_v2, torch.optim.Adam(simple_cnn_v2.parameters(), lr=0.1), epochs=5, device=\"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3iRtH60UT3u",
        "outputId": "891a9c06-e411-4240-be72-f4c559093944"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss 0.1902, Train accuracy 0.9411 Val loss 0.0976, Val accuracy 0.9698\n",
            "Epoch 2: Train loss 0.0928, Train accuracy 0.9714 Val loss 0.0783, Val accuracy 0.9754\n",
            "Epoch 3: Train loss 0.0689, Train accuracy 0.9788 Val loss 0.0650, Val accuracy 0.9787\n",
            "Epoch 4: Train loss 0.0587, Train accuracy 0.9820 Val loss 0.0650, Val accuracy 0.9808\n",
            "Epoch 5: Train loss 0.0565, Train accuracy 0.9821 Val loss 0.0523, Val accuracy 0.9842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_validate(simple_cnn_v3, torch.optim.Adam(simple_cnn_v3.parameters(), lr=0.1), epochs=5, device=\"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vv1M9eUYS8N",
        "outputId": "86199832-21e1-4d26-8d88-b545d4579f62"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss 0.2018, Train accuracy 0.9367 Val loss 0.1156, Val accuracy 0.9629\n",
            "Epoch 2: Train loss 0.0939, Train accuracy 0.9704 Val loss 0.0811, Val accuracy 0.9745\n",
            "Epoch 3: Train loss 0.0749, Train accuracy 0.9768 Val loss 0.0673, Val accuracy 0.9782\n",
            "Epoch 4: Train loss 0.0641, Train accuracy 0.9807 Val loss 0.0618, Val accuracy 0.9795\n",
            "Epoch 5: Train loss 0.0571, Train accuracy 0.9823 Val loss 0.0516, Val accuracy 0.9839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model with real images\n",
        "xb, __ = next(iter(dls))\n",
        "num_images = 5\n",
        "with torch.no_grad():\n",
        "  for i in range(num_images):\n",
        "    print(\"Image:\")\n",
        "    show_image(xb[i])\n",
        "    plt.show()\n",
        "    preds = simple_cnn(xb[i]) # CNN takes in [batch_size, 1, 28, 28]\n",
        "    #print(f\"Prediction: {preds.argmax()} \\n List of Predictions: {preds}\")\n",
        "    print(f\"Prediction: {preds.argmax()}\")"
      ],
      "metadata": {
        "id": "F1ShEEU9F_Nj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}