{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODC+pputhNnBv6MfMq5QSy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexlinapp/python_tools_practice/blob/main/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5qut2iKf0FXg",
        "outputId": "eef558eb-fc18-45c7-ff80-eb6b6a43cb6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.7.9)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Collecting fsspec==2023.6.0\n",
            "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2023.6.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2023.6.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.9)\n",
            "Downloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.1\n",
            "    Uninstalling transformers-4.53.1:\n",
            "      Successfully uninstalled transformers-4.53.1\n",
            "Successfully installed transformers-4.53.2\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install fsspec==2023.6.0\n",
        "!pip install --upgrade transformers tokenizers\n",
        "# !pip uninstall -y torch torchtext\n",
        "# !pip install torch==2.0.1+cu117 torchtext==0.15.2 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N8ZgiK1xXNl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import load_dataset\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "from collections import Counter\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "from transformers import GPT2Tokenizer\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import TensorDataset\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "lC-fIGkP1XEo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a872ce8c-bb50-4e20-b285-10d309c5b77d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# This should load without issues\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429,
          "referenced_widgets": [
            "57064331b7754defbb8e9fe629d5f248",
            "effcdb416f0c46a594d559af49984aed",
            "4cfb6cdce4564129b363e3450996680d",
            "7c1bec512bc94ec9815a0727ea1d0e2d",
            "a93eb4186f6d4308be0c78df5902fe1f",
            "453b767099d64739b6202883f2759b03",
            "c798ec3f87e04a5d9da794afa8f3b257",
            "5b5e1aaf59cf4d9f8825b7bf4f30aef6",
            "e04b1955ecf444ac8e1b8001a0d70f3c",
            "83c120c124064460b4d8bdc6acba4e46",
            "4c3c1cd82eb84592b93220beba03b0c2",
            "d576600b0d8c487da79f124e1d99907d",
            "879be7bca80d49ef8a73b8f280466f07",
            "5efbdc4bcb5945759c7236349cea50c5",
            "3085e09ea0c743778ae0d2b8807d4284",
            "8aa427994c0c4a458b406964b2daa1dd",
            "9ca4e6fb980d49ecbb163d980c120ddd",
            "18484b3faff346c492bd51c3110aa382",
            "dc905111f7f046edac6aa923bc1f8b16",
            "b9a816306150446eaf409e95ad5ee5a5",
            "b3cda8566bd143a398770d06856a0bf6",
            "26af6945527240249ab18ffda662f7d1",
            "a34264d48dac48639eff5030fdd40028",
            "55a786c61be24d9f82387fa640808fbe",
            "ce7a248e337d470abb0fac2fd17e88f6",
            "e42955cac1bb4fde9fe9382b71fe9af2",
            "7435b137848042698e3dc81662da31a4",
            "0e8f9218baa64394b47d056e3f174f4b",
            "4895feedca3f4954997edc761a966b47",
            "0868a21bf8d34cfda75bc0819d340274",
            "4d5c3a7f92964634963178c75fcbaf5a",
            "bdc9cab3d139409297be6ae48c126df3",
            "f228689a6e1941938115d813a371ddf3",
            "2df78704ead34629ba03f9bb655e2fc0",
            "42413f63c21e402895515f8fa247e695",
            "8eb2c94e957e44f4b72405d8a5934853",
            "7783eb531b3548d08e47d794e388e218",
            "e681e5549ce54b4dace8a5a3c50a5992",
            "6940f47f63a14f00b6770394fcd959a2",
            "83968460ecf44621b801823cd88afaf3",
            "bc17b68ecd4e454d9cfd0b95dab2d440",
            "9d5f22566a074bd99c9e2683f30e35ee",
            "143f3d8476794116a9defeec7fb78b94",
            "a5323d6161f14bcfb2f085f0b13208d6",
            "07acca5245f1494b85bdd8156d9a7409",
            "837fa240fe254d53ad5bc057bee2dfbb",
            "ca9bfe23071e4a15bf9e9cb39daf3d0a",
            "851f253767524b42880bac4e76590a0f",
            "40e148e9aa01427cab69d793d3403066",
            "1d863776ede74ea195d648c2e8c41ab9",
            "9f08f2e54ec04ad3b267166cd072ed2f",
            "24c4a69650e3476aad515efd16073a50",
            "fe0ccf81b8794f0e9d0558b5ffd52fa9",
            "6b78822f4d6141b39ca29eb4357bbd65",
            "4b25475c5fae4150b736a963db52924e",
            "dbb1167606504462954c822aa34cc25e",
            "848543d8df30443ba24e9be9aecbf8f3",
            "6cec9c53b3c14853a27adee35594431e",
            "d3421795bbac45f3a2fc094b06af6fdb",
            "b99df3e6169646e38f9219ca46fca6b0",
            "8de3ded0c91e4ee0a06cc0c4b72b95b6",
            "76d7ed4acafd434aa751e30ae1e9d02c",
            "f2c254856ac046a393c6999ed7157f1f",
            "fbf2c0f3af06474d8174b41dc0515444",
            "4c54df31a7d04e3cad9fdd327f6141f2",
            "7be36e4665c6458a8aae284fe69a87d4",
            "206f985fc1c946688760324b6048a922",
            "49f5f18069684448b6e92666ea45cf2c",
            "a9e92c5bf7b3405a9fc82223447e3e8a",
            "58a7aa20ff3745e59f187fda7ffc8947",
            "9184681da3a94a23b5dac8d4a5cffd77",
            "520a1a009c86405cb1bfebac80813669",
            "0ba2e30d8bde431f9cb3a35dbf848082",
            "5e75cf1c17e64e62b9601bdd30ae327f",
            "85b645c9642849578044b17eea1274a2",
            "2b99c26f1e234f8b8a67527dde22f32e",
            "b4232b3b199e42ebba31319295e450d9",
            "8ccc4e34b355459684e35ae215c141f0",
            "a6294212651741ec8f9e20f9e11fe887",
            "2c803f2417c44462a5cd1a359dc7dd34",
            "ea594010158e4f058267f43ec90a3d98",
            "3db378f0416d45c58fcb4bc7ec06dedd",
            "65503476b3b8424ab5b4ee52ef04591e",
            "958dc847b1294fa5bda74e3413a07bc8",
            "b81198a2572544aca013bc4b646c84cc",
            "c87bc90c2cc64cd28ac00037e193a6ca",
            "3715f0d262b644ca8c689e7f4162769e",
            "fd43ddcf25f64358839741564619014d",
            "8ed44cd4aefc4e38975cbb82ffe8ade9",
            "6c87c901d9a847f09ef5d21104f7b7ff",
            "cae0dac640a04d4cbdbd74d726c22e99",
            "f82bc2898e4b4300a62a92b706b024a9",
            "d1dd9b3f52d040979a6083280a2880e9",
            "fe06f2b97ae14cd2a6da831909b3c768",
            "0f645ba2350740fc95597ad11bc24d03",
            "c566e5963b1f43e6ab91b654a6318b92",
            "c6261c6bf8e24fc69adeeaa60164c4b5",
            "ae66e31194f64f5c96f46fed8b37bdb2",
            "2849a1b7585f49c087438c8260e75d6d"
          ]
        },
        "id": "nadYbgXlAoXz",
        "outputId": "2540607b-cf23-421b-ddfc-ccb1d5985a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57064331b7754defbb8e9fe629d5f248"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d576600b0d8c487da79f124e1d99907d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/685k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a34264d48dac48639eff5030fdd40028"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/6.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2df78704ead34629ba03f9bb655e2fc0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/618k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07acca5245f1494b85bdd8156d9a7409"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbb1167606504462954c822aa34cc25e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "206f985fc1c946688760324b6048a922"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ccc4e34b355459684e35ae215c141f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ed44cd4aefc4e38975cbb82ffe8ade9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2TokenizerFast\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"])\n",
        "\n",
        "tokenized = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n"
      ],
      "metadata": {
        "id": "IDp3FZ4bBncy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273,
          "referenced_widgets": [
            "6d4e3e2886ae432da14b7aaf68cbd5ab",
            "f9b3163ed3be4e0d83b0e20341ece4a2",
            "2978d05fa2dc474484da2017420bb0d6",
            "9ff7be587b5247c781bd9153e0dc4c2b",
            "9899859bfd6b49aeb434439e312c082c",
            "9bc9cf0a23614ca3a258dd0a6ee48fff",
            "e32592fef98e4583a97bef4e53eb7cb1",
            "ae9e21e6bda54c32a3ace62be56429e3",
            "b915366963904e9093f1f20b11aa5870",
            "d92a9f6833d548238adec2429111adb7",
            "1d79e0a449154113a111e37a3537f3d9",
            "910abe2f333b476a821d570aae6a3a23",
            "4b086c5bf63d4672ba7a1cd63d7bbebf",
            "b9942b77923e46268d8a63ae7a25ab6c",
            "8dcd90c65b9b428ea2f57dc35070c5bd",
            "bc69a95692144d61a4510bd8c85b58b0",
            "57325f238ddc4020a3c266737f9ea2f8",
            "cfc1e03601bc467cb9cb54ea8ef4bb41",
            "b4d8cdd5a53d4ed19cbef0569a67e4a6",
            "b2940b6a9caa4a3cb93bc313d0ca10a5",
            "f447d26d73c94efbaae6d333b541ff63",
            "89b519627fbd4b58a74e871cf5b4733a",
            "006b3fd21060412d88ff9101322d7b06",
            "c24e218084e64d02a3cb3a031fbdb3ec",
            "467dd80eab2f43a9872feb10ffc50c2d",
            "d49c52d852f14ac8b145221f87c186b8",
            "4be32e46cc914d21be109ca7915e0a16",
            "cb0a650663fd44129eeac5ed37834daa",
            "8fc3ddbeb6214f7e8c0a7217b735dc9d",
            "278aff0fa59245079d308c2707cbfdf6",
            "52329d556d4f4730ae1c75f41204a020",
            "162311108d0f4482923e112ac8e89e10",
            "96c8645d49224b77ac0f0a522452733b",
            "c9a0b0c1e38b49a1830f7f6d49667792",
            "9cb5619148c546fd854d34a136a754ed",
            "42a382bf2b804830ac4e327e6c31a214",
            "657dc1beb93e464fbb01667a9d6945ab",
            "3204976928244b42ae0bf87371d60e42",
            "e5c28dc70a9844a6b582143f3138072a",
            "9fe67e2669eb4124804fcbd6c9ebe7fa",
            "81f9ab582ab3425db804895f072a7bbf",
            "8f7843122762471ba213361bdedc6574",
            "0b3c60190daa47ad8191eefde41bb4f7",
            "1f7be52a4cf14d47a9b18826c38fe681",
            "35319fac376f410290adf8677f606638",
            "c35828c00c144bc8b25560fd806eb3a6",
            "56a21d279b3a404ca025226276ad8c28",
            "502865df6b5d4becbfe89f8cb483167f",
            "57ffc4f2785a4b36b3e8c51dcd5db437",
            "4f993266180348309ebe4e3d2c0c8b51",
            "8ebd893a0c18469f831cb1779b12fb49",
            "abbdeaf1cfa84036b7fd75673e9e682e",
            "b1aa1205e187443c9fbea85f292cc989",
            "1ea87814f67f4db287b31a5c408e6c32",
            "c636f676f9624b29824ae34cf4f68f0a",
            "c0f5ac01f7ba4a1e9433d3ad38565a73",
            "2e308f5e262f4b859be06213457807ec",
            "3e1cf770e14144aeb96a21c42e8f1373",
            "3212b47d11124699ae2f09f512582383",
            "a5498fb85aa2490d8f6d53cb37535c93",
            "6c48bee2483c4229980470fa96c4ac37",
            "5238fc9536a541bfada81df363851a6a",
            "6a35e21b851243a99b764821770d4452",
            "96be830ff820410d9b7a20b4eed4ace2",
            "0c0df763aed442c5b7d09db8c3294efc",
            "3f5c15517f0f425ab649f1f794a4f950",
            "574f2953cfba450b9ad4f7661b54c44a",
            "31707a685d0941c5b1319266c73d010c",
            "25970a547d80449098b2d09b2a68dd56",
            "eb012fb25ee640ed8576a69607667584",
            "f50c6880f6314320a2d86420a8dc766e",
            "1662270aa8244b72ac3e760cd902c483",
            "ebcdd1f8a7bd4c858ef74b09bcc4090a",
            "d9e60f39e30647378215d01a7f5bef5a",
            "3130c3757aeb459ba6f033c220b560d5",
            "f8fac1c7dfc6408e89701e4d44267bab",
            "1c844f8c3d7849039873a2838fed268d",
            "e80c02f7107f42faa143e89b481c3513",
            "8aa80064b60445eda1df0b090f9fd471",
            "9ca9e966c4d3484093b23f67542ef997",
            "07d5a5aab5ab46a99d0ed13e32e6b282",
            "7eaa1f96a5e2401babcd7e38e85e21ec",
            "86eb1c1e964e4e6c9f9542e01aecde9d",
            "dc8dccdab0d84b84839e8e2cff79a9dd",
            "5a28355305ff4affa9b818912e5314cd",
            "90608616cd5b4e72b35b3a9ffa4d2e07",
            "b72fc8e4169a4f8391893aa9157e3c56",
            "79cba7b6f4224850a6ac2b07ccb2b2ab"
          ]
        },
        "outputId": "15ca7222-857e-4b79-fa4a-be45c456e7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d4e3e2886ae432da14b7aaf68cbd5ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "910abe2f333b476a821d570aae6a3a23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "006b3fd21060412d88ff9101322d7b06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9a0b0c1e38b49a1830f7f6d49667792"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35319fac376f410290adf8677f606638"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0f5ac01f7ba4a1e9433d3ad38565a73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "574f2953cfba450b9ad4f7661b54c44a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e80c02f7107f42faa143e89b481c3513"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenized['train']['input_ids']\n",
        "print(type(tokenized['train']))\n",
        "\n",
        "from itertools import chain\n",
        "\n",
        "# Suppose 'tokenized' is your tokenized DatasetDict or Dataset\n",
        "\n",
        "# Extract the list of token lists (input_ids) from your train split\n",
        "input_ids_lists = tokenized['train']['input_ids']  # list of lists of ints\n",
        "\n",
        "# Use itertools.chain to flatten efficiently into one big list\n",
        "all_input_ids = list(chain.from_iterable(input_ids_lists))\n"
      ],
      "metadata": {
        "id": "q7x5uOSsUE9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b8a7954-398e-4db8-fc3c-3e86a3e0be9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'datasets.arrow_dataset.Dataset'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def group_strides_tensor(seq, bs):\n",
        "  n = seq.size(0) // bs\n",
        "  x = seq[:bs * n].reshape(bs, n, -1)\n"
      ],
      "metadata": {
        "id": "4sdRiOpjyhXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tokens = torch.tensor(all_input_ids)[:200000]\n",
        "bs = 64\n",
        "seq_len = 5;\n",
        "x = input_tokens.unfold(0, seq_len, 1)[:-1] # drops the last window\n",
        "y = input_tokens[seq_len:]\n",
        "\n",
        "tokendataset = TensorDataset(x, y)\n",
        "dataloader = DataLoader(tokendataset, batch_size=bs, shuffle=True, drop_last=True)\n",
        "first = next(iter(dataloader))\n",
        "first[0].shape, first[1].shape\n",
        "m1 = LMModel1(vocab_sz=tokenizer.vocab_size, n_hidden=16, seq_len = seq_len, n_layers=1)\n",
        "#m1(first[0]).shape\n",
        "# grouping by strides\n",
        "print(first[0].shape)\n",
        "m2 = LSTMCell(16, 16, tokenizer.vocab_size, 5, \"cuda\")\n",
        "m2(first[0]).shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gnlapSlZM6Y",
        "outputId": "8c1cdda3-cfb3-400e-a24f-a27c4b75c976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 5])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel1(nn.Module):\n",
        "  def __init__(self, vocab_sz, n_hidden, seq_len, n_layers=1):\n",
        "    super().__init__()\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden) # Embedded layer. Maps inputs (vocab) to vector\n",
        "    self.h_h = nn.Linear(n_hidden, n_hidden)    # Hidden layer\n",
        "    self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "    self.h_o = nn.Linear(n_hidden, vocab_sz)    # Output layer\n",
        "    self.seq_len = seq_len\n",
        "    self.h = torch.zeros(n_layers, bs, n_hidden).to(\"cuda\")\n",
        "  def forward(self, x):\n",
        "    # for i in range(self.seq_len):\n",
        "    #   self.h = self.h + self.i_h(x[:,i])\n",
        "    #   #print(self.h.shape)\n",
        "    #   self.h = F.relu(self.h_h(self.h))\n",
        "    #   out = self.h_o(self.h)\n",
        "    #   print(out.shape)\n",
        "    #   self.h = self.h.detach()\n",
        "    # return out\n",
        "    res,h = self.rnn(self.i_h(x), self.h)\n",
        "    #print(res.shape)\n",
        "    self.h = h.detach()\n",
        "    #print(res.shape)\n",
        "    return self.h_o(res[:,-1,:])\n",
        "  def reset(self): self.h = 0\n",
        "\n",
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, ni, nh, vocab_size, seq_len, device):\n",
        "        super().__init__()\n",
        "        self.ih = nn.Linear(ni, 4 * nh)\n",
        "        self.hh = nn.Linear(nh, 4 * nh)\n",
        "        self.i_he = nn.Embedding(vocab_size, ni)\n",
        "        self.h_o = nn.Linear(nh, vocab_size)\n",
        "        self.seq_len = seq_len\n",
        "        self.h = torch.zeros(bs, nh).to(device)\n",
        "        self.c = torch.zeros(bs, nh).to(device)\n",
        "    def forward(self, input):\n",
        "        for i in range(self.seq_len):\n",
        "          input_step = self.i_he(input[:,i])\n",
        "          gates = (self.ih(input_step) + self.hh(self.h)).chunk(4, 1)\n",
        "          ingate, forgetgate, outgate = map(torch.sigmoid, gates[:3])\n",
        "          cellgate = gates[3].tanh()\n",
        "          self.c = (forgetgate * self.c) + (ingate * cellgate)\n",
        "          self.h = outgate * self.c.tanh()\n",
        "        self.h = self.h.detach()\n",
        "        self.c = self.c.detach()\n",
        "        return self.h_o(self.h)"
      ],
      "metadata": {
        "id": "QJ9aCGXLKRC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MVcr6zlZ3jSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, optimizer, loss_fnc, device, epochs=4):\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_tokens = 0\n",
        "    print(\"entering\")\n",
        "    for xb, yb in dataloader:\n",
        "      xb = xb.to(device)\n",
        "      yb = yb.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      out = model(xb)        # shape: (batch_size, sequence_length)\n",
        "      #print(out.shape)\n",
        "      loss = loss_fnc(out, yb)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss.item()\n",
        "      total_correct += (out.argmax(dim=1) == yb).sum().item()\n",
        "      total_tokens += xb.shape[0]\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = total_correct / total_tokens\n",
        "    print(f\"Epoch {epoch+1}/{epochs} — Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "_Z19m5R22EiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LMModel1(vocab_sz=tokenizer.vocab_size, n_hidden=32, seq_len = seq_len, n_layers=4)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fnc = nn.CrossEntropyLoss()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_model(model, dataloader, optimizer, loss_fnc, device, epochs=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "47sFvHnGiuKM",
        "outputId": "e99917b8-f818-47e5-c7a5-28409ab32978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "entering\n",
            "Epoch 1/4 — Loss: 6.7006, Accuracy: 0.1374\n",
            "entering\n",
            "Epoch 2/4 — Loss: 5.9176, Accuracy: 0.1877\n",
            "entering\n",
            "Epoch 3/4 — Loss: 5.6553, Accuracy: 0.2004\n",
            "entering\n",
            "Epoch 4/4 — Loss: 5.4581, Accuracy: 0.2112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = LSTMCell(16, 16, tokenizer.vocab_size, seq_len, device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fnc = nn.CrossEntropyLoss()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_model(model2, dataloader, optimizer, loss_fnc, device, epochs=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "zL1t7NB2Pwxy",
        "outputId": "6b5c8efe-bb87-415e-8817-e69fdfcfe253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "entering\n",
            "Epoch 1/4 — Loss: 10.8542, Accuracy: 0.0000\n",
            "entering\n",
            "Epoch 2/4 — Loss: 10.8542, Accuracy: 0.0000\n",
            "entering\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-195-516808489.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloss_fnc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fnc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-122-149011544.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, optimizer, loss_fnc, device, epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# shape: (batch_size, sequence_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m       \u001b[0;31m#print(out.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fnc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-194-561653687.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m           \u001b[0minput_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi_he\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m           \u001b[0mgates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mih\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m           \u001b[0mingate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforgetgate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m           \u001b[0mcellgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start = time.time()\n",
        "for xb, yb in dataloader:\n",
        "    pass\n",
        "print(f\"DataLoader iteration took {time.time()-start:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scVpfCyFlqDV",
        "outputId": "0f38e641-73bd-4221-d3cb-e6559dd0923e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoader iteration took 19.69 seconds\n"
          ]
        }
      ]
    }
  ]
}