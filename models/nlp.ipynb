{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVIDnLumLEEl37snGTrXAq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexlinapp/python_tools_practice/blob/main/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5qut2iKf0FXg",
        "outputId": "06ee5274-5849-43c7-b4b5-acec72abad4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.7.9)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Collecting fsspec==2023.6.0\n",
            "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2023.6.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2023.6.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.9)\n",
            "Downloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.1\n",
            "    Uninstalling transformers-4.53.1:\n",
            "      Successfully uninstalled transformers-4.53.1\n",
            "Successfully installed transformers-4.53.2\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install fsspec==2023.6.0\n",
        "!pip install --upgrade transformers tokenizers\n",
        "# !pip uninstall -y torch torchtext\n",
        "# !pip install torch==2.0.1+cu117 torchtext==0.15.2 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N8ZgiK1xXNl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import load_dataset\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "from collections import Counter\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "from transformers import GPT2Tokenizer\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import TensorDataset\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "lC-fIGkP1XEo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "141597fc-4aea-431f-9326-c5d05b4550ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# This should load without issues\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429,
          "referenced_widgets": [
            "6e61a187a3d14dc0ba32b94767feb68b",
            "a6b88a8d488a426b954abfd7004a2b96",
            "62f4be37861948c1a6e296dfbcf83a66",
            "2a7186426b4a4fafb8f16fa10196fd9b",
            "b6d22bd396234705afdd97ac09f46e48",
            "0f459f099a614c91b2570b33ad8c0b1b",
            "728a53cbe8464eee9e4f45ab191abbd5",
            "ed24ac4bb12e4d9b8a13ec795cd5440e",
            "27250e8c88ae4370a1b6bb6dc5312c9a",
            "ed2a2529fbaf4e8986d8f45c04359cdf",
            "6631c995c0f947b9af23fdadc56e8ce2",
            "458c3f8260f04cd1a7c994ff057e599c",
            "9e38c8522e91436eae621fd6e9bc2a39",
            "d379842d594b43f38ec2f6c888cce95f",
            "92a35d320fdd4ba4bb7fa4b013eeac9d",
            "89f5cbcd6fa548afb2e0db4739925e3f",
            "4b5a055b81c6441da5c99f45434159f3",
            "ff7cc615d3b943758b56783e33d4994c",
            "876bf3fdf7d24eb5b5ea8801268ed668",
            "af2eb357f4e2470da36785c4698fafb6",
            "eeba3638d4864f0aa29817a4ea19dab1",
            "617369e00a40444ba34a7b48fdae4b00",
            "7b2b40a833684b7d822e2d577b83fa3b",
            "9c77d589088a455d97bf7d0891a004b6",
            "47355ad0e78845a58888c11c3226597d",
            "9d1d2a994da84149bd98d00a770b8116",
            "13dab9b3586644489261d4e289a30e8e",
            "24bf85e6b54640d6a2b9697bf13ac86a",
            "ea676938a51348c0a3becb73d754a80f",
            "0c00ab3e64544d4abaab0069e32c3daf",
            "649023ec15d14739a147a1e508ef309a",
            "131ddadd6ceb418988e24c502914360a",
            "191830764a904238bc58a9a504dd6433",
            "44e623ca8dd14f44b3276de202ac29ac",
            "8cb127d6c00f4c6ca221024de9a4dd90",
            "04f5327071e943e98ba864470fbe2d6f",
            "02d51b9630f94e13b781faa8de5ca987",
            "e6a66f74c6c4429ebe6547956ddd4f1e",
            "2c08b15a217c44919176ca7244af708b",
            "cf80ca99d0934b31bc923673b1668dbf",
            "2b2db07cf9794cc8a89f958af0232525",
            "4c8986e427344401af30cdde31b165f6",
            "903fb97905f64603bafa679335080196",
            "1c0db453586c465e83dc16feaab12bc1",
            "43252bc39169445a9381314e4003714e",
            "39f1a35fb26e448087bfc4321a51a9e7",
            "78899e2432cc493bb66d9d90368ec6a1",
            "fc07bef815684d9b9c078ab13bebcf5e",
            "a730e5074a9e4a2eb7b94107e50eda84",
            "821f2873f74a43feab70c5e49b767a6a",
            "b0e8249a5b2343489f3e15cc732a612a",
            "40575f56d55d4af18b730dc48c6fc725",
            "36c3dda1a6814037b1e1c6a29f3cae3f",
            "32cc1c243f014e19b54cf1ce8189faca",
            "228629fe40084811b626069d77f47510",
            "25e2ea5c258b41a5ae482e689c867df0",
            "b51a0b1fd1234b1c88e63cbdc17b81ac",
            "2d4d585d8c504fc8b555d35fad197a56",
            "5cb4da503eb54cdbb4a76efddd5a0b37",
            "4f9e75b1ddff4a2999d5942c66719175",
            "47217d1f35804a34a6d1c2acb5e45f26",
            "e27bfb5432ef4a69a79445bce78b7298",
            "24906d3755474eedb85428bdcf079498",
            "464ab41e037a4cb48dbe4f32eafe2aa7",
            "c96bbe9ab50b4f688237b164b462b231",
            "8dd20c0a20674235bcb0f91c0050920a",
            "fcd8eb99fa264263ba1dd675adc10d7d",
            "af256e23262c4d35bac5ff2afd161aee",
            "838d43f340ba4e1eb0b7aa276496b20d",
            "9d721ceefdc247ad8d8c972cd1866248",
            "cd817dfdebdb47148e8c94a83d160424",
            "f922739303404a0ea54c5ba25b01b953",
            "65b3a2e2658945f9a1ddba985a1bcdf3",
            "8e8bc796afd2450dacb3354a8b2498d9",
            "fc9db61ab50042b1afe9b10fd18d8ccc",
            "31d501098c2948dd8d99fadfd128963e",
            "18924a60c233414fb911e96a8dd1f826",
            "b08159babb814211aded8f7a2204f3fe",
            "b513ff909f0841e98d4b4a979993cdb5",
            "bc6b191ac4314cd98e10a9adb5c0e17d",
            "674000c156c94d0b9ed580ec06a8e292",
            "c77a82cd1188418fb0ed20a0c49f7c97",
            "04df6c41a8724d43ad068e1527b4ef3f",
            "1ceb210916194d14a424e0ede46b6af6",
            "8d7152d7f9ef46c59afd8a00a0db0456",
            "b45834f42cfe48ec80752b2aad175886",
            "a07c50bdd07f4a5fbb3047be89ad9b21",
            "0517c22410874bbfaf7d2ad1ea5533b0",
            "57eff6a8c10f41cb95225f355770e71d",
            "29694f93527649fd9270e5b9e611aebb",
            "e3e6614196164d159bfd831beca7ab12",
            "cc1510548c4d4eb6b90b59c7be9f7cda",
            "faaf089425f6438aa632dbf13d080383",
            "aa788d3710e048efb0d5b738410ff845",
            "f55b9f54fcc04797a89e16d84daa05e3",
            "d1bbce532b33425eb5129f4d516c9f49",
            "10575c33b80240be96ccce11125a1580",
            "c5b444124b774a98833ecd67068e7f0a",
            "e958087a62f8476588224db3ecd39a0b"
          ]
        },
        "id": "nadYbgXlAoXz",
        "outputId": "0aeb5eee-1faf-489f-f900-e1ed7de0235d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e61a187a3d14dc0ba32b94767feb68b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "458c3f8260f04cd1a7c994ff057e599c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/685k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b2b40a833684b7d822e2d577b83fa3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/6.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44e623ca8dd14f44b3276de202ac29ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/618k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43252bc39169445a9381314e4003714e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25e2ea5c258b41a5ae482e689c867df0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fcd8eb99fa264263ba1dd675adc10d7d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b08159babb814211aded8f7a2204f3fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57eff6a8c10f41cb95225f355770e71d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2TokenizerFast\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"])\n",
        "\n",
        "tokenized = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n"
      ],
      "metadata": {
        "id": "IDp3FZ4bBncy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273,
          "referenced_widgets": [
            "4b0ad4c5f6b64ac193566f217da21db9",
            "286e174b56134c6a9f16583b9cddfa16",
            "cc06d1ad5e7546f9ad025007a067daeb",
            "3fecd568ab80453dbc8d0d119cc85158",
            "20db9ab315ee4083ac7dd18702e8f054",
            "8eb7bd8491634e42a5f44a29b9db5179",
            "1b52ea2e43a541e7afb1cddc1549f341",
            "fba3440d66cc4f5d81ebb3ee40c8fe80",
            "fb918035401845e88641b65d53fb483b",
            "9358f84f41d54fba80da9b12b167c5ac",
            "38aeffcc26c84fcca02fd7a64ab6fb60",
            "7fea40cec36c46318a4bb7492d252a24",
            "7a2ed7d8ee81431aa20f3879e4306594",
            "181964c9b17944619eeec30c68f26557",
            "2ebe97849ccb4093b6db26ae51322ee0",
            "2937e2167c254dbabb98fe1d0c87adab",
            "83461ba102d4433782c0a7ed1571234f",
            "a71126d487924f8289be37f03377c017",
            "68e246dc6ebe49108a093035ab39a620",
            "ce386aad1ca04f0e857777bc30548c0b",
            "c606722576e14f02bae6d870917dd9c3",
            "775bfe5c8bea4639b7eaf038ded1698a",
            "c7e89999209544539ff745965afe8b3b",
            "f805cc2ee64a478fa3e81a11762a8098",
            "66666a9db29c46f9a98bc48220266135",
            "b4a98b5833974b3ebffeb603cf2211f1",
            "e971c0c049604152b556962b96820c72",
            "bbd8472eaed9478f9b3a0fadbc3580b9",
            "754915e2c23548dd8921fabb4fa3b86f",
            "9028d6604bb8455ea445e4d7927ae936",
            "e0bade25f3dd415bb0ebe88d350ac2f3",
            "ca2537a755a74c01ac714911f6b0bdec",
            "ebf7ff32a54a43108ecdea03ee2582a9",
            "2618aaffa3b444619747e4e6b503b246",
            "66aee46ea5844b86b387058352172825",
            "f03902c686dd46f8b557dd7179124a7c",
            "f0aff7ee1b414859b9054a44012ca038",
            "65c7c14a21ae4c13840a61fed0741b43",
            "ccbc6ea466f945ebbf780fb64399445f",
            "4552440320804d2b9cd564990588ad27",
            "4b4d0fbfe6674ab3a55c3cb4a9b590d8",
            "db3df8011a9d4c1393f6ba892f48692d",
            "7286dd0d4f8f41d1b6b63ae7abad2dda",
            "8fc8c5526404466ea58c4e36633edd8f",
            "5d9a34e05f4543c48839fc27fcf54781",
            "8b60d67c01aa4c1f851fbb6fbe57a31a",
            "5e53c340fefc40d19833f5407f05c30f",
            "a1f2d09d3fcd435cabc6bf090a99b9db",
            "e2e466402e0b475fa3c9d819cb7bd11c",
            "b3a29f42a6c347449e34f23f9ec878c1",
            "fd4317adbafe4f7495e91f9a17a2d6dc",
            "4681ab748eaf4c8daa6ffec10d66d498",
            "c668e7efec9e481b94a2db568109aba0",
            "50a0cf8e783b421fb027954a8bd27b5d",
            "092806ed8a30416bad9337d66857632d",
            "0a4b434d04c34742858c0daf2ebf7585",
            "d0df1431a49341b28a7791d33abd6a3b",
            "6cd13281f0c344f98e25ce50ef5be577",
            "bb22f8956b7c4a2abfb5918826c6e852",
            "24226c8b14e643e980270d163da9165d",
            "739bf830fb1245b7bbd90fe7a0d5baa7",
            "95520bee0c4044759a1427832e53e584",
            "cebada24cc5944e5b502397b67185ce8",
            "b3b6ea98448f4f0082427d1f2489e0af",
            "921a961ffbbb4c108c67e76ecd286663",
            "d663dc5b3ccb480493e13890dbf7d617",
            "d0d00b33dfe842b596cae3f73bf177e7",
            "414792cb585947898f6322a75cdf009d",
            "a49ee577ff7040d3a451d67c4f7c11e0",
            "2578721264914a6baa71a65a6b5ef429",
            "fe1690aad5fe4d7c8461669082c403b2",
            "16b99d8221c94e07aaa54b3fe2e9b071",
            "6470809ace82484e81910dbeeeb5778e",
            "39ea09c4d3e2479e8b5995ea0f3045ea",
            "7aaddf7445334bdf86aa7508523944fe",
            "bcf2efe2b4ee416dbfe563c2d8d843f7",
            "f015d248425c499cbbfbd28f9987d28e",
            "8021b0171f24477c859b1538d095c4a0",
            "1bdee37f3b724227b433bdcc009ad914",
            "0e5606c20a994b0ba7dd88ca27c02894",
            "ac225a6907cf4c818c0c0eb6f259c1bb",
            "d6e8e7abb7e34f0f88f2f7a295cbe5f9",
            "dd311bc36d19470186e28f870c17f333",
            "bacbd2cc718640ada7eb1ebacd23e39c",
            "971bf97519004cf8afdf2b29bc97aad1",
            "37b146d915614d71aff4e65fc5982889",
            "2d6ba8292399445fb779950d7e0aef66",
            "abe5ae9967dd4f2ea89005174de12302"
          ]
        },
        "outputId": "b3bb4459-7209-4cbb-e134-67a311aa1752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b0ad4c5f6b64ac193566f217da21db9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7fea40cec36c46318a4bb7492d252a24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7e89999209544539ff745965afe8b3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2618aaffa3b444619747e4e6b503b246"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d9a34e05f4543c48839fc27fcf54781"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a4b434d04c34742858c0daf2ebf7585"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0d00b33dfe842b596cae3f73bf177e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8021b0171f24477c859b1538d095c4a0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenized['train']['input_ids']\n",
        "print(type(tokenized['train']))\n",
        "\n",
        "from itertools import chain\n",
        "\n",
        "# Suppose 'tokenized' is your tokenized DatasetDict or Dataset\n",
        "\n",
        "# Extract the list of token lists (input_ids) from your train split\n",
        "input_ids_lists = tokenized['train']['input_ids']  # list of lists of ints\n",
        "\n",
        "# Use itertools.chain to flatten efficiently into one big list\n",
        "all_input_ids = list(chain.from_iterable(input_ids_lists))\n"
      ],
      "metadata": {
        "id": "q7x5uOSsUE9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efd02f99-ec67-4247-a441-0738f2d53ed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'datasets.arrow_dataset.Dataset'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def group_strides_tensor(seq, bs):\n",
        "  n = seq.size(0) // bs\n",
        "  x = seq[:bs * n].reshape(bs, n, -1)\n"
      ],
      "metadata": {
        "id": "4sdRiOpjyhXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tokens = torch.tensor(all_input_ids)[:200000]\n",
        "bs = 64\n",
        "seq_len = 5;\n",
        "x = input_tokens.unfold(0, seq_len, 1)[:-1] # drops the last window\n",
        "y = input_tokens[seq_len:]\n",
        "\n",
        "windows = input_tokens.unfold(0, seq_len + 1, 1)[:-1]\n",
        "x1 = windows[:, :-1]\n",
        "y1 = windows[:, 1:]\n",
        "tokendataset1 = TensorDataset(x1, y1)\n",
        "tokendataset = TensorDataset(x, y)\n",
        "dataloader = DataLoader(tokendataset, batch_size=bs, shuffle=True, drop_last=True)\n",
        "dataLoader1 = DataLoader(tokendataset1, batch_size=bs, shuffle=True, drop_last=True)\n",
        "first = next(iter(dataloader))\n",
        "first[0].shape, first[1].shape\n",
        "m1 = LMModel1(vocab_sz=tokenizer.vocab_size, n_hidden=16, seq_len = seq_len, n_layers=1)\n",
        "m1(first[0])\n",
        "# grouping by strides\n",
        "#print(first[0].shape)\n",
        "m2 = LSTMCell(16, 16, tokenizer.vocab_size, 5, bs, \"cpu\")\n",
        "m2(first[0]).shape\n",
        "#m2(first[0]).shape\n",
        "first1 = next(iter(dataLoader1))\n",
        "first1[0][:4], first1[1][:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gnlapSlZM6Y",
        "outputId": "8b0432bd-e4fc-4f23-fca8-d304b88e921a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 1279,  2954,    29,   656,   262],\n",
              "         [12566,   286,   262, 17757,   290],\n",
              "         [25747,  3170,  1141, 27256,   764],\n",
              "         [  632,  6774,   262,  9173,   736]]),\n",
              " tensor([[ 2954,    29,   656,   262,  5386],\n",
              "         [  286,   262, 17757,   290,  8161],\n",
              "         [ 3170,  1141, 27256,   764, 27182]]))"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel1(nn.Module):\n",
        "  def __init__(self, vocab_sz, n_hidden, seq_len, n_layers=1, device=\"cpu\"):\n",
        "    super().__init__()\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden) # Embedded layer. Maps inputs (vocab) to vector\n",
        "    self.h_h = nn.Linear(n_hidden, n_hidden)    # Hidden layer\n",
        "    self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "    self.h_o = nn.Linear(n_hidden, vocab_sz)    # Output layer\n",
        "    self.seq_len = seq_len\n",
        "    self.h = 0#torch.zeros(n_layers, bs, n_hidden).to(\"cuda\")\n",
        "    self.device = device\n",
        "  def forward(self, x):\n",
        "\n",
        "    for i in range(self.seq_len):\n",
        "      self.h = self.h + self.i_h(x[:,i])\n",
        "      #print(self.h.shape, x[:,i].shape)\n",
        "      self.h = F.relu(self.h_h(self.h))\n",
        "      out = self.h_o(self.h)\n",
        "      #print(out.shape)\n",
        "    self.h = self.h.detach()\n",
        "    return out\n",
        "    # res,h = self.rnn(self.i_h(x), self.h)\n",
        "    # #print(res.shape)\n",
        "    # self.h = h.detach()\n",
        "    # #print(res.shape)\n",
        "    # return self.h_o(res[:,-1,:])\n",
        "  def reset(self): self.h = 0\n",
        "\n",
        "\n",
        "class LMModel2(nn.Module):\n",
        "  def __init__(self, vocab_sz, n_hidden, seq_len, n_layers=1, device=\"cpu\"):\n",
        "    super().__init__()\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden) # Embedded layer. Maps inputs (vocab) to vector\n",
        "    self.h_h = nn.Linear(n_hidden, n_hidden)    # Hidden layer\n",
        "    self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "    self.h_o = nn.Linear(n_hidden, vocab_sz)    # Output layer\n",
        "    self.seq_len = seq_len\n",
        "    self.h = 0#torch.zeros(n_layers, bs, n_hidden).to(\"cuda\")\n",
        "    self.device = device\n",
        "  def forward(self, x):\n",
        "    outputs = torch.zeros(x.size(0), self.seq_len, self.h_o.out_features, device=self.device)\n",
        "    for i in range(self.seq_len):\n",
        "      self.h = self.h + self.i_h(x[:,i])\n",
        "      #print(self.h.shape, x[:,i].shape)\n",
        "      self.h = F.relu(self.h_h(self.h))\n",
        "      out = self.h_o(self.h)\n",
        "      #print(out.shape)\n",
        "      outputs[:, i, :] = out\n",
        "    self.h = self.h.detach()\n",
        "    return outputs.permute(0, 2, 1)\n",
        "    # res,h = self.rnn(self.i_h(x), self.h)\n",
        "    # #print(res.shape)\n",
        "    # self.h = h.detach()\n",
        "    # #print(res.shape)\n",
        "    # return self.h_o(res[:,-1,:])\n",
        "  def reset(self): self.h = 0\n",
        "\n",
        "\n",
        "\n",
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, embed, nh, vocab_size, seq_len, bs, device):\n",
        "        super().__init__()\n",
        "        self.forget_gate = nn.Linear(embed + nh, nh)\n",
        "        self.input_gate  = nn.Linear(embed + nh, nh)\n",
        "        self.cell_gate   = nn.Linear(embed + nh, nh)\n",
        "        self.output_gate = nn.Linear(embed + nh, nh)\n",
        "        self.i_he = nn.Embedding(vocab_size, embed)\n",
        "        self.h_o = nn.Linear(nh, vocab_size)\n",
        "        self.seq_len = seq_len\n",
        "        self.h = torch.zeros(bs, nh).to(device)\n",
        "        self.c = torch.zeros(bs, nh).to(device)\n",
        "        self.nh = nh\n",
        "        self.bs = bs\n",
        "        self.device = device\n",
        "    def forward(self, x):\n",
        "        self.h = self.h.detach()\n",
        "        self.c = self.c.detach()\n",
        "        # self.h = torch.zeros(self.bs, self.nh).to(self.device)\n",
        "        # self.c = torch.zeros(self.bs, self.nh).to(self.device)\n",
        "        for i in range(self.seq_len):\n",
        "          input = self.i_he(x[:,i])\n",
        "          temp_h = torch.cat([self.h, input], dim=1)\n",
        "          #print(\"temph: \", temp_h.shape)\n",
        "          forget = torch.sigmoid(self.forget_gate(temp_h))\n",
        "          #print(\"forget: \", forget.shape)\n",
        "          forget_c = self.c * forget\n",
        "          #print(\"self.c: \", self.c.shape)\n",
        "          #print(\"forget_c: \", forget_c.shape)\n",
        "          inp = torch.sigmoid(self.input_gate(temp_h))\n",
        "          cell = torch.tanh(self.cell_gate(temp_h))\n",
        "          self.c = forget_c + inp * cell\n",
        "          out = torch.sigmoid(self.output_gate(temp_h))\n",
        "          self.h = out * torch.tanh(self.c)\n",
        "          #print(\"self.h: \", self.h)\n",
        "          #print(\"self.c: \", self.c)\n",
        "        return self.h_o(self.h)\n",
        "\n",
        "    # def __init__(self, ni, nh, vocab_size, seq_len, device):\n",
        "    #     super().__init__()\n",
        "    #     self.ih = nn.Linear(ni, 4 * nh)\n",
        "    #     self.hh = nn.Linear(nh, 4 * nh)\n",
        "    #     self.i_he = nn.Embedding(vocab_size, ni)\n",
        "    #     self.h_o = nn.Linear(nh, vocab_size)\n",
        "    #     self.seq_len = seq_len\n",
        "    #     # self.h = torch.zeros(bs, nh).to(device)\n",
        "    #     # self.c = torch.zeros(bs, nh).to(device)\n",
        "    # def forward(self, input):\n",
        "    #     for i in range(self.seq_len):\n",
        "    #       input_step = self.i_he(input[:,i])\n",
        "    #       gates = (self.ih(input_step) + self.hh(self.h)).chunk(4, 1)\n",
        "    #       ingate, forgetgate, outgate = map(torch.sigmoid, gates[:3])\n",
        "    #       cellgate = gates[3].tanh()\n",
        "    #       self.c = (forgetgate * self.c) + (ingate * cellgate)\n",
        "    #       self.h = outgate * self.c.tanh()\n",
        "    #     self.h = self.h.detach()\n",
        "    #     self.c = self.c.detach()\n",
        "    #     return self.h_o(self.h)"
      ],
      "metadata": {
        "id": "QJ9aCGXLKRC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, optimizer, loss_fnc, device, epochs=4):\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_tokens = 0\n",
        "    print(\"entering\")\n",
        "    for xb, yb in dataloader:\n",
        "      xb = xb.to(device)\n",
        "      yb = yb.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      out = model(xb)        # shape: (batch_size, sequence_length)\n",
        "      #print(out.shape)\n",
        "      loss = loss_fnc(out, yb)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss.item()\n",
        "      total_correct += (out.argmax(dim=1) == yb).sum().item()\n",
        "      total_tokens += xb.shape[0]\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = total_correct / total_tokens\n",
        "    print(f\"Epoch {epoch+1}/{epochs} — Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "_Z19m5R22EiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = next(iter(dataloader))\n",
        "modeltest = LSTMCell(16, 32, tokenizer.vocab_size, seq_len, bs, \"cpu\")\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(modeltest.parameters(), lr=0.001)\n",
        "loss = loss_func(modeltest(xb), yb)\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "# for param in modeltest.parameters():\n",
        "#   print(param.grad)\n",
        "print(loss)\n",
        "for j in range(1):\n",
        "  print(xb[:5])\n",
        "  print(yb[:5])\n",
        "  optimizer.zero_grad()\n",
        "  out = modeltest(xb)\n",
        "  loss = loss_func(out, yb)\n",
        "  print(loss)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "i = 0\n",
        "\n",
        "\n",
        "modeltest = LMModel2(vocab_sz=tokenizer.vocab_size, n_hidden=32, seq_len = seq_len, n_layers=4)\n",
        "\n",
        "for xb, yb in dataLoader1:\n",
        "  optimizer.zero_grad()\n",
        "  out = modeltest(xb)\n",
        "  loss = loss_func(out, yb)\n",
        "  print(loss)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  i += 1\n",
        "  if i == 1000:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "N5f_xtyItqgC",
        "outputId": "b63c3d68-b4d6-4728-a822-99f4e0368743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10.8392, grad_fn=<NllLossBackward0>)\n",
            "tensor([[  796,   569, 18354,  7496, 17740],\n",
            "        [  569, 18354,  7496, 17740,  6711],\n",
            "        [18354,  7496, 17740,  6711,   796],\n",
            "        [ 7496, 17740,  6711,   796,   220],\n",
            "        [17740,  6711,   796,   220,   198]])\n",
            "tensor([6711,  796,  220,  198, 2311])\n",
            "tensor(10.8309, grad_fn=<NllLossBackward0>)\n",
            "tensor(10.8608, grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(10.9029, grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(10.9609, grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(10.8912, grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(10.8952, grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(10.9020, grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(10.8965, grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(10.8299, grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(10.9337, grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(10.9692, grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(10.8907, grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(10.9456, grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(10.8499, grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(10.8760, grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(10.8770, grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(10.8629, grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(10.8840, grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(10.9342, grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(10.8721, grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(10.8442, grad_fn=<NllLoss2DBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-139-3309127980.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LMModel1(vocab_sz=tokenizer.vocab_size, n_hidden=32, seq_len = seq_len, n_layers=4, device=device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fnc = nn.CrossEntropyLoss()\n",
        "print(device)\n",
        "train_model(model, dataloader, optimizer, loss_fnc, device, epochs=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "47sFvHnGiuKM",
        "outputId": "ddb529d9-d05d-4641-be8f-eddaea7988a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "entering\n",
            "Epoch 1/4 — Loss: 6.6838, Accuracy: 0.1473\n",
            "entering\n",
            "Epoch 2/4 — Loss: 5.8001, Accuracy: 0.1773\n",
            "entering\n",
            "Epoch 3/4 — Loss: 5.4283, Accuracy: 0.1971\n",
            "entering\n",
            "Epoch 4/4 — Loss: 5.1291, Accuracy: 0.2142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = LMModel2(vocab_sz=tokenizer.vocab_size, n_hidden=32, seq_len = seq_len, n_layers=4, device=device)\n",
        "optimizer3 = torch.optim.Adam(model3.parameters(), lr=0.001)\n",
        "train_model(model3, dataLoader1, optimizer3, loss_fnc, device, epochs=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwVsXNbI82gM",
        "outputId": "a685c54a-9e6b-4aee-f474-1def7c0a19fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "entering\n",
            "Epoch 1/4 — Loss: 6.1012, Accuracy: 0.8238\n",
            "entering\n",
            "Epoch 2/4 — Loss: 5.1752, Accuracy: 1.0623\n",
            "entering\n",
            "Epoch 3/4 — Loss: 4.7780, Accuracy: 1.1929\n",
            "entering\n",
            "Epoch 4/4 — Loss: 4.5508, Accuracy: 1.2826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = LSTMCell(32, 32, tokenizer.vocab_size, seq_len, bs, device)\n",
        "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
        "loss_fnc = nn.CrossEntropyLoss()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_model(model2, dataloader, optimizer2, loss_fnc, device, epochs=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL1t7NB2Pwxy",
        "outputId": "77e2bb91-8dd4-4400-a071-43dd82bef7f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "entering\n",
            "Epoch 1/4 — Loss: 6.7505, Accuracy: 0.1440\n",
            "entering\n",
            "Epoch 2/4 — Loss: 5.9308, Accuracy: 0.1865\n",
            "entering\n",
            "Epoch 3/4 — Loss: 5.5658, Accuracy: 0.2031\n",
            "entering\n",
            "Epoch 4/4 — Loss: 5.2715, Accuracy: 0.2213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start = time.time()\n",
        "for xb, yb in dataloader:\n",
        "    pass\n",
        "print(f\"DataLoader iteration took {time.time()-start:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scVpfCyFlqDV",
        "outputId": "0f38e641-73bd-4221-d3cb-e6559dd0923e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoader iteration took 19.69 seconds\n"
          ]
        }
      ]
    }
  ]
}